{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"workshop-analytics\") \\\n",
    "    .config(\"spark.master\", \"spark://sparkmaster:7077\")\\\n",
    "    .config(\"spark.cassandra.connection.host\", \"node1\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = spark.read\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"generation\", keyspace=\"energydata\")\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@10b23498 [region#0,type#1,ts#2,value#3] ReadSchema: struct<region:string,type:string,ts:timestamp,value:double>\n"
     ]
    }
   ],
   "source": [
    "ct.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ctpd = ct.filter(\"ts < cast('2012-01-11' as timestamp)\")\n",
    "#ctpd.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "ct_agg = ct \\\n",
    "    .withColumn('year', year(ct.ts)) \\\n",
    "    .withColumn('month', month(ct.ts)) \\\n",
    "    .filter(\"type == 'solar' AND region == 'DE'\") \\\n",
    "    .groupBy('type', 'region', 'year', 'month') \\\n",
    "    .agg( \\\n",
    "        max(\"value\").alias(\"max_generation_MW\"),\n",
    "        sum(col(\"value\")/(4*10**3)).alias(\"sum_generation_GWh\") # divide by 4*10^3 because we have 15 min MW values\n",
    "        ) \\\n",
    "    .withColumn('sum_generation_GWh', round('sum_generation_GWh', 0)) \\\n",
    "    .sort(desc('sum_generation_GWh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sort [sum_generation#214 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(sum_generation#214 DESC NULLS LAST, 200)\n",
      "   +- *HashAggregate(keys=[type#1, region#0, year#186], functions=[avg(value#3), max(value#3), sum(value#3)])\n",
      "      +- Exchange hashpartitioning(type#1, region#0, year#186, 200)\n",
      "         +- *HashAggregate(keys=[type#1, region#0, year#186], functions=[partial_avg(value#3), partial_max(value#3), partial_sum(value#3)])\n",
      "            +- *Project [region#0, type#1, value#3, year(cast(ts#2 as date)) AS year#186]\n",
      "               +- *Filter (isnotnull(type#1) && isnotnull(region#0))\n",
      "                  +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@10b23498 [region#0,type#1,value#3,ts#2] PushedFilters: [IsNotNull(type), IsNotNull(region), *EqualTo(type,solar), *EqualTo(region,DE)], ReadSchema: struct<region:string,type:string,value:double,year:int>\n"
     ]
    }
   ],
   "source": [
    "ct_agg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-----+-----------------+------------------+\n",
      "| type|region|year|month|max_generation_MW|sum_generation_GWh|\n",
      "+-----+------+----+-----+-----------------+------------------+\n",
      "|solar|    DE|2013|    7|          23998.0|            5129.0|\n",
      "|solar|    DE|2016|    7|          25688.0|            4943.0|\n",
      "|solar|    DE|2015|    7|          24731.0|            4918.0|\n",
      "|solar|    DE|2014|    6|          24244.0|            4834.0|\n",
      "|solar|    DE|2016|    6|          26201.0|            4767.0|\n",
      "|solar|    DE|2016|    8|          25371.0|            4720.0|\n",
      "|solar|    DE|2016|    5|          26252.0|            4717.0|\n",
      "|solar|    DE|2015|    8|          24429.0|            4613.0|\n",
      "|solar|    DE|2015|    6|          24847.0|            4553.0|\n",
      "|solar|    DE|2015|    4|          25928.0|            4435.0|\n",
      "|solar|    DE|2014|    7|          23624.0|            4417.0|\n",
      "|solar|    DE|2015|    5|          22453.0|            4412.0|\n",
      "|solar|    DE|2013|    6|          23203.0|            4313.0|\n",
      "|solar|    DE|2012|    5|          22402.0|            4146.0|\n",
      "|solar|    DE|2013|    8|          22903.0|            4144.0|\n",
      "|solar|    DE|2014|    5|          23515.0|            4107.0|\n",
      "|solar|    DE|2014|    8|          21959.0|            3895.0|\n",
      "|solar|    DE|2016|    9|          24377.0|            3848.0|\n",
      "|solar|    DE|2012|    8|          20570.0|            3839.0|\n",
      "|solar|    DE|2016|    4|          25898.0|            3752.0|\n",
      "+-----+------+----+-----+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "#ct_agg.explain()\n",
    "%time ct_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct_station = spark.read\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"weather_station\", keyspace=\"energydata\")\\\n",
    ".load()\n",
    "\n",
    "ct_sensor = spark.read\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"weather_sensor\", keyspace=\"energydata\")\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_subset = ct_station\\\n",
    ".filter(\"lat < 50 and lon > 10\") \n",
    "\n",
    "sensor_subset = ct_sensor.filter(\"sensor=='h2'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2248704"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_subset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_subset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining with the Datasets API is limited, joins are not pushed down\n",
    "\n",
    "You would need to go with the RDD API (Scala only) using rdd.joinWithCassandraTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 44.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "351360"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "%time station_subset.join(sensor_subset, station_subset.id == sensor_subset.id).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*SortMergeJoin [id#247], [id#254], Inner\n",
      ":- *Sort [id#247 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(id#247, 200)\n",
      ":     +- *Filter ((((isnotnull(lon#249) && isnotnull(lat#248)) && isnotnull(id#247)) && (lat#248 < 50.0)) && (lon#249 > 10.0))\n",
      ":        +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@7a27bb71 [id#247,lat#248,lon#249] PushedFilters: [IsNotNull(lon), IsNotNull(lat), IsNotNull(id), LessThan(lat,50.0), GreaterThan(lon,10.0)], ReadSchema: struct<id:string,lat:double,lon:double>\n",
      "+- *Sort [id#254 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(id#254, 200)\n",
      "      +- *Filter ((isnotnull(sensor#255) && (sensor#255 = h2)) && isnotnull(id#254))\n",
      "         +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@d32519b [id#254,sensor#255,ts#256,value#257] PushedFilters: [IsNotNull(sensor), EqualTo(sensor,h2), IsNotNull(id)], ReadSchema: struct<id:string,sensor:string,ts:timestamp,value:double>\n"
     ]
    }
   ],
   "source": [
    "station_subset.join(sensor_subset, station_subset.id == sensor_subset.id).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27887381.5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.11549526E8/4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
