{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"workshop-analytics\") \\\n",
    "    .config(\"spark.master\", \"spark://sparkmaster:7077\")\\\n",
    "    .config(\"spark.cassandra.connection.host\", \"node1\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct = spark.read\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"generation\", keyspace=\"energydata\")\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@785f32c2 [region#9,type#10,ts#11,value#12] ReadSchema: struct<region:string,type:string,ts:timestamp,value:double>\n"
     ]
    }
   ],
   "source": [
    "ct.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------------------+-------+\n",
      "|region|type|                 ts|  value|\n",
      "+------+----+-------------------+-------+\n",
      "|    DE|wind|2016-12-31 22:45:00|15236.0|\n",
      "|    DE|wind|2016-12-31 22:30:00|15074.0|\n",
      "|    DE|wind|2016-12-31 22:15:00|14997.0|\n",
      "|    DE|wind|2016-12-31 22:00:00|14915.0|\n",
      "|    DE|wind|2016-12-31 21:45:00|14880.0|\n",
      "|    DE|wind|2016-12-31 21:30:00|14876.0|\n",
      "|    DE|wind|2016-12-31 21:15:00|14818.0|\n",
      "|    DE|wind|2016-12-31 21:00:00|14816.0|\n",
      "|    DE|wind|2016-12-31 20:45:00|15294.0|\n",
      "|    DE|wind|2016-12-31 20:30:00|15449.0|\n",
      "|    DE|wind|2016-12-31 20:15:00|15391.0|\n",
      "|    DE|wind|2016-12-31 20:00:00|15284.0|\n",
      "|    DE|wind|2016-12-31 19:45:00|15292.0|\n",
      "|    DE|wind|2016-12-31 19:30:00|15268.0|\n",
      "|    DE|wind|2016-12-31 19:15:00|15290.0|\n",
      "|    DE|wind|2016-12-31 19:00:00|15361.0|\n",
      "|    DE|wind|2016-12-31 18:45:00|15371.0|\n",
      "|    DE|wind|2016-12-31 18:30:00|15251.0|\n",
      "|    DE|wind|2016-12-31 18:15:00|15126.0|\n",
      "|    DE|wind|2016-12-31 18:00:00|15034.0|\n",
      "+------+----+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ctpd = ct.filter(\"ts < cast('2012-01-11' as timestamp)\")\n",
    "#ctpd.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "ct_agg = ct \\\n",
    "    .withColumn('year', year(ct.ts)) \\\n",
    "    .withColumn('month', month(ct.ts)) \\\n",
    "    .filter(\"type == 'solar' AND region == 'DE'\") \\\n",
    "    .groupBy('type', 'region', 'year', 'month') \\\n",
    "    .agg( \\\n",
    "        max(\"value\").alias(\"max_generation_MW\"),\n",
    "        sum(col(\"value\")/(4*10**3)).alias(\"sum_generation_GWh\") # divide by 4*10^3 because we have 15 min MW values\n",
    "        ) \\\n",
    "    .withColumn('sum_generation_GWh', round('sum_generation_GWh', 0)) \\\n",
    "    .sort(desc('sum_generation_GWh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ct_agg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-----+-----------------+------------------+\n",
      "| type|region|year|month|max_generation_MW|sum_generation_GWh|\n",
      "+-----+------+----+-----+-----------------+------------------+\n",
      "|solar|    DE|2013|    7|          23998.0|            5129.0|\n",
      "|solar|    DE|2016|    7|          25688.0|            4943.0|\n",
      "|solar|    DE|2015|    7|          24731.0|            4918.0|\n",
      "|solar|    DE|2014|    6|          24244.0|            4834.0|\n",
      "|solar|    DE|2016|    6|          26201.0|            4767.0|\n",
      "|solar|    DE|2016|    8|          25371.0|            4720.0|\n",
      "|solar|    DE|2016|    5|          26252.0|            4717.0|\n",
      "|solar|    DE|2015|    8|          24429.0|            4613.0|\n",
      "|solar|    DE|2015|    6|          24847.0|            4553.0|\n",
      "|solar|    DE|2015|    4|          25928.0|            4435.0|\n",
      "|solar|    DE|2014|    7|          23624.0|            4417.0|\n",
      "|solar|    DE|2015|    5|          22453.0|            4412.0|\n",
      "|solar|    DE|2013|    6|          23203.0|            4313.0|\n",
      "|solar|    DE|2012|    5|          22402.0|            4146.0|\n",
      "|solar|    DE|2013|    8|          22903.0|            4144.0|\n",
      "|solar|    DE|2014|    5|          23515.0|            4107.0|\n",
      "|solar|    DE|2014|    8|          21959.0|            3895.0|\n",
      "|solar|    DE|2016|    9|          24377.0|            3848.0|\n",
      "|solar|    DE|2012|    8|          20570.0|            3839.0|\n",
      "|solar|    DE|2016|    4|          25898.0|            3752.0|\n",
      "+-----+------+----+-----+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%time ct_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ct_station = spark.read\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"weather_station\", keyspace=\"energydata\")\\\n",
    ".load()\n",
    "\n",
    "ct_sensor = spark.read\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"weather_sensor\", keyspace=\"energydata\")\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_subset = ct_station\\\n",
    ".filter(\"lat < 50 and lon > 10\") \n",
    "\n",
    "sensor_subset = ct_sensor.filter(\"sensor=='h2'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2248704"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_subset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_subset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining with the Datasets API is limited, joins are not pushed down\n",
    "\n",
    "You would need to go with the RDD API (Scala only) using rdd.joinWithCassandraTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 29.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "351360"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "%time station_subset.join(sensor_subset, station_subset.id == sensor_subset.id).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_subset.join(sensor_subset, station_subset.id == sensor_subset.id).explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
